---
permalink: /
title: "About"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a research scientist at the Tongyi Lab, Alibaba Group. I obtained the Ph.D. in *Artificial Intelligence* from the College of Computer Science and Technology, __*Zhejiang University*__ (ZJU), China, supervised by Prof. [Chao Wu](https://wuchaozju.github.io/). I was a visiting researcher at _**the University of Cambridge**_ under the supervision of Prof. [Nicholas Lane](https://niclane.org/). I am honored to work with Prof. [Tao Lin](https://lins-lab.github.io/) at Westlake University/EPFL. 

<!--I am the Associate Chair of [FedKDD 2025](https://fedkdd.github.io/fedkdd2025/) (Federated Learning Workshop of KDD 2025). During my Ph.D., I also earned a minor degree in Technology-based (BEST) MBA from the School of Management, ZJU. Before Ph.D., I received my bachelor's degree in *Engineering* from ZJU; meanwhile, I obtained an *honorary undergraduate degree* from Chu Kochen Honors College of ZJU and a minor degree in Public Affairs (social governance).-->

<!--My main research interests lie in *__i) optimization, generalization, and personalization of deep learning models__*, especially under *distributed/federated/collaborative* setups, which are generally empowered by deep learning phenomena and mechanistic interpretability;  *__ii) trustworthy model manipulation for foundation models__*: understanding and improving foundation models (e.g., large language model, vision transformer, and diffusion transformer) from the *__model parameter perspective__*, i.e., model *fusion, editing, pruning, stitching, growth, unlearning, and generation*.-->

My main research interests lie in the following aspects. __*Large Language models and agentic intelligence.*__ i. Model editing and
memory management for large language models (LLMs). ii. LLM agent and reasoning. iii. Parametric understanding of LLMs (localization, merging, scaling, pruning, stitching, unlearning,
editing, and etc.) iv. Text-to-model generation. 
__*Trustworthy deep learning.*__ i. Privacy-preserving federated learning (FL), efficient & robust
algorithm design, and generalization, personalization & training dynamics understanding. ii.
Mechanistic interpretability of neural networks, weight decay, loss landscape, permutation
invariance, linear mode connectivity, and etc. iii. Socio-technical issues brought by collaborative
learning. iv. Responsible and trustworthy AI.


## Contact
Email: zexi.li\[at\]zju.edu.cn / tomleeze\[at\]gmail.com 

Wechat and Phone: (+86) 18868104540

<!--
## Research Interests
* **Collaborative Deep Learning**
  * *__Collaborative foundation models__*: LLM agents collaborated with humans, the Web, and each other; collaborative finetuning foundation models (LLMs, diffusion models, etc.).
  * *__Federated deep Learning__*: algorithm design in terms of *generalization*, *personalization*, *robustness*, and *efficiency* & *training dynamics* understanding.
  * *__Edge-cloud collaborative__* & *domain-transferred* machine learning: *real-world applications under constrained resources* (efficiency, data availability, etc.).
  * *__Trustworthy__* perspectives of machine learning: *privacy, robustness, fairness, reliability, and interpretability*.
  * *__Socio-technical issues__* brought by *big data* and *machine learning*.

* **Mechanistic Understanding of Deep Learning**
  * *__Behind mechanisms of collaborative deep learning__*: *model fusion*, *permutation invariance*, and *linear mode connectivity*.
  * *__Generalization__*: initialization, weight decay regularization, federated training, etc.
  * *__Deep learning phenomena__*: neural collapse, grokking, neural scaling law, etc.
  * Understanding and improving *__LLMs__*.-->

<!--[**Preprint**] [Text-to-Model: Text-Conditioned Neural Network Diffusion for Train-Once-for-All Personalization](https://arxiv.org/pdf/2405.14132)  
  **Zexi Li\***, Lingzhi Gao\*, and Chao Wu\# --> 

## Selected Publications
- [**NeurIPS 2024**] [WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of Large Language Models](https://arxiv.org/pdf/2405.14768)  
  Peng Wang\*, **Zexi Li\***, Ningyu Zhang\#, Ziwen Xu, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, and Huajun Chen\#
- [**Preprint**] [Editing as Unlearning: Are Knowledge Editing Methods Strong Baselines for Large Language Model Unlearning?](https://arxiv.org/abs/2505.19855)  
   **Zexi Li\***, Xiangzhu Wang\*, William F. Shen, Meghdad Kurmanji, Xinchi Qiu, Dongqi Cai, Chao Wu\#, Nicholas D. Lane\#  
- [**KDD 2025**] [FedGuCci: Making Local Models More Connected in Landscape for Federated Learning](https://arxiv.org/pdf/2402.18949.pdf)  
  **Zexi Li\***, Jie Lin\*, Zhiqi Li\*, Didi Zhu, Tao Shen, Tao Lin\#, Chao Wu\#, Nicholas D. Lane  
- [**ICML 2023**] [Revisiting Weighted Aggregation in Federated Learning with Neural Networks](https://proceedings.mlr.press/v202/li23s.html)  
  **Zexi Li**, Tao Lin\#, Xinyi Shang, and Chao Wu\#  
- [**Patterns, Cell Press**] [Can We Share Models If Sharing Data Is Not an Option?](https://www.cell.com/patterns/fulltext/S2666-3899(22)00228-8#%20)  
  **Zexi Li**, Feng Mao\#, and Chao Wu\#
- [**ICCV 2023**] [No Fear of Classifier Biases: Neural Collapse Inspired Federated Learning with Synthetic and Fixed Classifier](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_No_Fear_of_Classifier_Biases_Neural_Collapse_Inspired_Federated_Learning_ICCV_2023_paper.pdf)  
  **Zexi Li**, Xinyi Shang, Rui He, Tao Lin\#, and Chao Wu\#  
<!--- [**ACM SIGIR 2023**] [Edge-cloud Collaborative Learning with Federated and Centralized Features](https://dl.acm.org/doi/abs/10.1145/3539618.3591976)  
  **Zexi Li\***, Qunwei Li\*, Yi Zhou, Wenliang Zhong\#, Guannan Zhang, and Chao Wu\#-->  
- [**IEEE Transactions on Big Data**] [Towards Effective Clustered Federated Learning: A Peer-to-peer Framework with Adaptive Neighbor Matching](https://arxiv.org/pdf/2203.12285.pdf)  
  **Zexi Li**, Jiaxun Lu, Shuang Luo, Didi Zhu, Yunfeng Shao, Yinchuan Li, Zhimeng Zhang, Yongheng Wang\#, and Chao Wu\#  

## Recent News
* **\[2025.07\]** I joined Tongyi Lab, Alibaba Group, as a research scientist.
* **\[2025.07\]** Our paper "Resource-Efficient Knowledge Editing for Mobile LLMs" has won the Best Poster Award at MobiUK 2025 in Edinburgh!
* **\[2025.07\]** I am invited to serve as the Session Chair of KDD 2025.
* **\[2025.06\]** I passed the PhD defense and graduated from the College of Computer Science and Technology, Zhejiang University.
* **\[2025.06\]** Our paper "[You Are Your Own Best Teacher: Achieving Centralized-level Performance in Federated Learning under Heterogeneous and Long-tailed Data](https://arxiv.org/abs/2503.06916)" is accepted by ICCV 2025!
* **\[2025.05\]** I am happy to share our new preprint "[Editing as Unlearning: Are Knowledge Editing Methods Strong Baselines for Large Language Model Unlearning?](https://arxiv.org/abs/2505.19855)"! We try to build a bridge connecting LLM editing and unlearning communities and find that editing methods are strong baselines for LLM unlearning to some extent.
* **\[2025.05\]** Our paper titled "FedGuCci: Making Local Models More Connected in Landscape for Federated Learning" is accepted by KDD 2025!
* **\[2025.04\]** Our paper titled "Towards Universal Personalization in Federated Learning via Collaborative Foundation Generative Models" is accepted by IEEE Transactions on Mobile Computing! Paper will be released soon.
* **\[2025.03\]** I will be the Associate Chair of [FedKDD 2025](https://fedkdd.github.io/fedkdd2025/), International Joint Workshop on Federated Learning for Data Mining and Graph Analytics, Co-located with the 31st ACM SIGKDD Conference (KDD 2025). Submissions are welcome!
* **\[2024.09\]** I am happy to share our [WISE](https://arxiv.org/pdf/2405.14768), a model editor for large language models' lifelong model editing, is accepted to NeurIPS 2024! 
* **\[2024.05\]** Two papers are accepted by KDD 2024, and one paper is accepted by ICML 2024. 

<!--* **\[2023.11\]** I am presented with *Guoqiang Scholarship of Zhejiang University* (Top 0.5%) and *First Prize of Chunxiao Scholarship of BEST-MBA* (Top 3%) by Zhejiang University.
* **\[2023.07\]** Two papers are accepted by _International Conference on Computer Vision (**ICCV 2023**)_! One is about federated learning and the other is about domain adaptation. Many thanks to my coauthors!
* **\[2023.06\]** We will have an oral presentation of our paper [FedCNI](https://arxiv.org/abs/2304.02892) at **ICME 2023** on July 11th.
* **\[2023.04\]** Our paper ["Revisiting Weighted Aggregation in Federated Learning with Neural Networks"](https://arxiv.org/abs/2302.10911) is accepted by _The Fortieth International Conference on Machine Learning (**ICML 2023**)_! Many thanks to my coauthors!
* **\[2023.04\]** Our paper "Edge-cloud Collaborative Learning with Federated and Centralized Features" is accepted by _The 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (**SIGIR 2023**)_! Thanks a lot to Ant Group's coauthors!
* **\[2023.03\]** Our paper "Learning Cautiously in Federated Learning with Noisy and Heterogeneous Clients" is accepted by _IEEE International Conference on Multimedia and Expo 2023 (**ICME 2023**)_! Congratulations to me and Chenrui!
* **\[2023.03\]** I have joined the **BEST-MBA** (**B**usiness, **E**ngineering, **S**cience, and **T**echnology based **MBA** for on-campus postgradutes) program of _School of Management, Zhejiang University_.--> 

<!--
* **\[2022.11\]** Our paper ["Towards Effective Clustered Federated Learning: A Peer-to-peer Framework with Adaptive Neighbor Matching"](https://www.computer.org/csdl/journal/bd/5555/01/09954190/1Inoq0EldXG)\[[arxiv](https://arxiv.org/pdf/2203.12285.pdf)\] is online now.
* **\[2022.11\]** Our paper ["Can we share models if sharing data is not an option?"](https://www.cell.com/patterns/fulltext/S2666-3899(22)00228-8#%20) is online now.
* **\[2022.11\]** Our paper "Towards Effective Clustered Federated Learning: A Peer-to-peer Framework with Adaptive Neighbor Matching" has been accepted by _IEEE Transactions on Big Data_! This paper had been through a dark time in the last year, and the acceptance means a lot to me. Congratulations to myself!
* **\[2022.10\]** I have passed *the Mid-term Assessment of Doctoral Program*.
* **\[2022.10\]** I am presented with *Outstanding Postgraduate Student Award* on 2021-2022 by Zhejiang University.
* **\[2022.09\]** Our new perspective paper "Can we share models if sharing data is not an option?" is accepted by _Patterns, Cell Press_. _Patterns_ is a child-journal about _data science_ of Cell Press, and it will have a first strong impact factor in 2023.-->

## Academic Service
* Associate Chair of [FedKDD 2025](https://fedkdd.github.io/fedkdd2025/), International Joint Workshop on Federated Learning for Data Mining and Graph Analytics, Co-located with the 31st ACM SIGKDD Conference (KDD 2025).
* Session Chair of KDD 2025.
* Invited Reviewers: TKDE, IJCV, TMM, Machine Learning, AISTATS 2024, CVPR 2024, ICML 2024 2025, NeurIPS 2024 2025, ICLR 2024 2025, KDD 2025, ICCV 2025.

## Talks
* \[2025.05.30\] 浙江省科协“科学+”平台；大模型时代，AI如何记忆与思考？
* \[2025.04.28\] Xtra Lab, National University of Singapore; Foundation Models under Model Parameter Perspective: Model Editing, Fusion, and Generation.
* \[2024.11.14\] Department of Computer Science and Technology, University of Cambridge; Physics in (Federated) Deep Neural Networks and Beyond: A Parametric Perspective.
